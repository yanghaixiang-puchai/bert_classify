{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3610jvsc74a57bd0d1bd47725ae3a7be53ee8a6e0905cf7661e13ffb6c4f9eef7655cf2e14fdde0b",
   "display_name": "Python 3.6.10 64-bit ('py36': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "d1bd47725ae3a7be53ee8a6e0905cf7661e13ffb6c4f9eef7655cf2e14fdde0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 0.导包"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json,time\n",
    "from  tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "from transformers import BertModel,BertConfig,BertTokenizer,AdamW,get_cosine_schedule_with_warmup\n",
    "\n",
    "#参数\n",
    "bert_path = 'bert_model/'   #预训练模型的位置\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)   #初始化分词器\n",
    "max_len = 30     #数据阻断长度\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 2"
   ]
  },
  {
   "source": [
    "# 1.预处理数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "180000it [01:58, 1524.94it/s]\n",
      "10000it [00:06, 1549.68it/s]\n",
      "10000it [00:06, 1548.40it/s]\n"
     ]
    }
   ],
   "source": [
    "#1.1处理数据成input_ids,token_type_ids,attention_mask,label\n",
    "def dataSet(data_path):\n",
    "    input_ids,token_type_ids,attention_mask = [],[],[]\n",
    "    labels = []\n",
    "    with open(data_path,encoding='utf-8') as f:\n",
    "        for i,line in tqdm(enumerate(f)):\n",
    "            title,y = line.strip().split('\\t')   #删除所有的空格，用\\t分割数据集和标签\n",
    "            #调用tokenizer转换成bert需要的数据格式\n",
    "            encode_dict = tokenizer.encode_plus(text=title,max_length=max_len,padding='max_length',truncation=True)\n",
    "            #分别获取三个值  目前值的类型为list\n",
    "            input_ids.append(encode_dict['input_ids'])\n",
    "            token_type_ids.append(encode_dict['token_type_ids'])\n",
    "            attention_mask.append(encode_dict['attention_mask'])\n",
    "            labels.append(int(y))\n",
    "    #list转化成tensor格式\n",
    "    input_ids,token_type_ids,attention_mask = torch.tensor(input_ids),torch.tensor(token_type_ids),torch.tensor(attention_mask)\n",
    "    return input_ids,token_type_ids,attention_mask,labels\n",
    "\n",
    "#1.2 dataloder批量处理\n",
    "def dataLoader(input_ids,token_type_ids,attention_mask,labels):\n",
    "    #tensor数据整合\n",
    "    labels = torch.tensor(labels)\n",
    "    data = TensorDataset(input_ids,token_type_ids,attention_mask,labels)\n",
    "    loader = DataLoader(data,batch_size=BATCH_SIZE,shuffle=True)    #shuffle打乱每行数据的顺序\n",
    "    return loader\n",
    "\n",
    "#1.3实例化函数\n",
    "#训练集带label\n",
    "input_ids_train,token_type_ids_train,attention_mask_train,labels_train = dataSet('data/train.txt')\n",
    "train_loader = dataLoader(input_ids_train,token_type_ids_train,attention_mask_train,labels_train)\n",
    "#验证集带label\n",
    "input_ids_dev,token_type_ids_dev,attention_mask_dev,labels_dev = dataSet('data/dev.txt')\n",
    "dev_loader = dataLoader(input_ids_dev,token_type_ids_dev,attention_mask_dev,labels_dev)\n",
    "#测试集 没有的话label放到dataloader\n",
    "# input_ids_test,token_type_ids_test,attention_mask_test,labels_test = dataSet('data/test.txt')\n",
    "# data = TensorDataset(input_ids_test,token_type_ids_test,attention_mask_test)\n",
    "# sample = RandomSampler(data) #随机采样\n",
    "# test_loader = DataLoader(data,sampler=sample,batch_size=BATCH_SIZE)\n",
    "#测试集\n",
    "input_ids_test,token_type_ids_test,attention_mask_test,labels_test = dataSet('data/test.txt')\n",
    "test_loader = dataLoader(input_ids_test,token_type_ids_test,attention_mask_test,labels_test)\n",
    "#得到后续用的数据为train_loader,dev_loader,test_loader"
   ]
  },
  {
   "source": [
    "# 2.定义bert模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self,bert_path,classes=10):\n",
    "        super(Bert_Model,self).__init__()\n",
    "        self.config = BertConfig.from_pretrained(bert_path)\n",
    "        self.bert = BertModel.from_pretrained(bert_path)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad=True\n",
    "        self.fc = nn.Linear(self.config.hidden_size,classes)  #直接分类\n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        output = self.bert(input_ids,token_type_ids,attention_mask)[1]  #池化后的输出,是向量\n",
    "        logit = self.fc(output)    #全连接层,概率矩阵\n",
    "        return logit\n",
    "\n",
    "#实例化bert模型\n",
    "model = Bert_Model(bert_path).to(DEVICE)"
   ]
  },
  {
   "source": [
    "# 3.定义优化器和线性学习率"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化器\n",
    "optimizer = AdamW(model.parameters(),lr=2e-5,weight_decay=1e-4)  #使用Adam优化器\n",
    "#设置学习率\n",
    "schedule = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=len(train_loader),num_training_steps=EPOCHS*len(test_loader))"
   ]
  },
  {
   "source": [
    "# 4. 定义训练函数和验证测试函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在验证集上评估模型性能的函数\n",
    "def evaluate(model,data_loader,device):\n",
    "    model.eval()   #防止模型训练改变权值\n",
    "    val_true,val_pred = [],[]\n",
    "    with torch.no_grad():     #计算的结构在计算图中,可以进行梯度反转等操作\n",
    "        for idx,(ids,tpe,att,y) in enumerate(data_loader): #得到的y要转换一下数据格式\n",
    "            y_pred = model(ids.to(device),tpe.to(device),att.to(device))  #此时得到的是概率矩阵\n",
    "            y_pred = torch.argmax(y_pred,dim=1).detach().cpu().numpy().tolist()  #将概率矩阵转换成标签并变成list类型\n",
    "            val_pred.extend(y_pred)   #将标签值放入列表\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())   #将真实标签转换成list放在列表中\n",
    "    \n",
    "    return accuracy_score(val_true,val_pred)\n",
    "#如果是比赛没有labels_test，那么这个函数for里面没有y，输出没有test_true，处理数据的时候没有labels_test放到dataloader里\n",
    "def predict(model,data_loader,device):\n",
    "    model.eval()\n",
    "    test_pred,test_true = [],[]\n",
    "    with torch.no_grad():\n",
    "        for idx,(ids,tpe,att,y) in enumerate(data_loader):\n",
    "            y_pred = model(ids.to(device),tpe.to(device),att.to(device))   #得到概率矩阵\n",
    "            y_pred = torch.argmax(y_pred,dim=1).detach().cpu().numpy().tolist()  #将概率矩阵转化成标签值\n",
    "            test_pred.extend(y_pred)\n",
    "            test_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "    return test_pred,test_true\n",
    "#训练函数\n",
    "def train_and_eval(model,train_loader,valid_loader,optimizer,schedule,device,epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()       #损失函数\n",
    "    for i in range(epoch):\n",
    "        start = time.time()\n",
    "        model.train()   #开始训练\n",
    "        print(\"***************我是狗Running training epoch{}************\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx,(ids,tpe,att,y) in enumerate(train_loader):\n",
    "            ids,tpe,att,y = ids.to(device),tpe.to(device),att.to(device),y.to(device)\n",
    "            y_pred = model(ids,tpe,att)   #加载模型获得概率矩阵\n",
    "            loss = criterion(y_pred,y)    #计算损失\n",
    "            optimizer.zero_grad()         #梯度清零\n",
    "            loss.backward()               #反向传播\n",
    "            optimizer.step()              #更新优化参数\n",
    "            schedule.step()               #更新学习率\n",
    "            train_loss_sum += loss.item()\n",
    "            #只打印五次结果\n",
    "            if(idx+1)%(len(train_loader)//5)==0:\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                i+1,idx+1,len(train_loader),train_loss_sum/(idx+1),time.time()-start))\n",
    "        #每一次epoch输出一个准确率\n",
    "        model.eval()\n",
    "        acc = evaluate(model,valid_loader,device)     #验证模型的性能\n",
    "        if acc > best_acc :\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(),\"best_bert_model.pth\")    #保存最好的模型\n",
    "        print(\"current acc is {:.4f},best acc is {:.4f}\".format(acc,best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time()-start,5)))"
   ]
  },
  {
   "source": [
    "# 5.开始训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "***************我是狗Running training epoch1************\n",
      "Epoch 0001 | Step 0562/2813 | Loss 1.2805 | Time 87.2074\n",
      "Epoch 0001 | Step 1124/2813 | Loss 0.8091 | Time 173.7297\n",
      "Epoch 0001 | Step 1686/2813 | Loss 0.6249 | Time 260.2511\n",
      "Epoch 0001 | Step 2248/2813 | Loss 0.5288 | Time 346.5547\n",
      "Epoch 0001 | Step 2810/2813 | Loss 0.4663 | Time 444.3679\n",
      "current acc is 0.9337,best acc is 0.9337\n",
      "time costed = 454.10747s \n",
      "\n",
      "***************我是狗Running training epoch2************\n",
      "Epoch 0002 | Step 0562/2813 | Loss 0.1625 | Time 86.6217\n",
      "Epoch 0002 | Step 1124/2813 | Loss 0.1633 | Time 173.0160\n",
      "Epoch 0002 | Step 1686/2813 | Loss 0.1606 | Time 259.7801\n",
      "Epoch 0002 | Step 2248/2813 | Loss 0.1590 | Time 360.3851\n",
      "Epoch 0002 | Step 2810/2813 | Loss 0.1581 | Time 459.5913\n",
      "current acc is 0.9418,best acc is 0.9418\n",
      "time costed = 469.2521s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_eval(model,train_loader,dev_loader,optimizer,schedule,DEVICE,EPOCHS)"
   ]
  },
  {
   "source": [
    "# 6.加载最优模型进行测试"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n Test Accuracy = 0.9492 \n \n              precision    recall  f1-score   support\n\n           0     0.9358    0.9470    0.9414      1000\n           1     0.9427    0.9710    0.9567      1000\n           2     0.9294    0.8950    0.9119      1000\n           3     0.9691    0.9710    0.9700      1000\n           4     0.9215    0.9270    0.9242      1000\n           5     0.9354    0.9560    0.9456      1000\n           6     0.9315    0.9240    0.9277      1000\n           7     0.9861    0.9910    0.9885      1000\n           8     0.9825    0.9540    0.9680      1000\n           9     0.9589    0.9560    0.9574      1000\n\n    accuracy                         0.9492     10000\n   macro avg     0.9493    0.9492    0.9491     10000\nweighted avg     0.9493    0.9492    0.9491     10000\n\n[1, 9, 4, 3, 3, 6, 3, 5, 7, 0]\n------------------\n[1, 9, 4, 3, 3, 6, 3, 5, 7, 0]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "#得到预测标签和真实标签\n",
    "test_pred,test_true= predict(model,test_loader,DEVICE)\n",
    "#输出测试机的准确率\n",
    "print(\"\\n Test Accuracy = {} \\n \".format(accuracy_score(test_true,test_pred)))\n",
    "#打印各项验证指标\n",
    "print(classification_report(test_true,test_pred,digits=4))\n",
    "print(test_pred[:10])\n",
    "print('------------------')\n",
    "print(test_true[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}